[92m[INFO] 2025-05-21 18:23:10 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:23:13 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:49:20 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:50:20 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:50:23 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:50:26 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-21 18:50:26 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-21 18:50:26 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-21 18:51:14 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:51:28 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:51:31 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-21 18:51:31 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-21 18:51:31 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-21 18:51:34 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:53:17 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:53:20 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-21 18:53:21 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-21 18:53:21 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-21 18:53:24 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:53:48 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:53:50 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-21 18:53:51 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-21 18:53:51 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-21 18:53:53 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:53:53 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:53:53 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-21 18:55:12 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:55:14 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-21 18:55:15 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-21 18:55:15 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-21 18:55:17 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:55:17 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:55:17 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-21 18:56:07 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:56:09 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-21 18:56:10 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-21 18:56:10 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-21 18:56:12 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:56:12 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:56:12 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-21 18:56:54 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:56:57 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-21 18:56:57 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-21 18:56:57 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-21 18:56:59 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:56:59 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 18:56:59 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-21 18:56:59 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-21 18:58:21 main:29 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-21 18:58:21 main:35 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-21 19:14:32 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 19:15:56 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 19:15:58 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-21 19:15:59 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-21 19:15:59 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-21 19:16:01 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 19:16:01 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-21 19:16:01 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-21 19:16:01 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-21 19:35:06 main:29 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-21 19:35:06 main:35 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-21 19:35:08 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 20:51:24 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 20:51:28 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-22 20:51:28 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-22 20:51:28 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-22 20:51:30 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 20:51:31 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 20:51:31 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-22 20:51:31 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-22 20:52:50 api.llm:25 - Запрос генерации: model=starcoder2-7b, user=string[0m
[92m[INFO] 2025-05-22 20:58:07 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 20:58:12 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-22 20:58:12 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-22 20:58:12 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-22 20:58:44 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 20:58:44 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 20:58:44 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-22 20:58:44 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-22 20:59:06 api.llm:25 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[91m[ERROR] 2025-05-22 20:59:20 api.llm:54 - Ошибка генерации
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\connection.py", line 207, in _new_conn
    raise ConnectTimeoutError(
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x0000026C86D20CD0>, 'Connection to huggingface.co timed out. (connect timeout=10)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "D:\intelligent-ai\venv\lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /deepseek-ai/deepseek-llm-7b-base/resolve/7683fea62db869066ddaff6a41d032262c490d4f/pytorch_model-00001-of-00002.bin (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000026C86D20CD0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1531, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1448, in get_hf_file_metadata
    r = _request_wrapper(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 309, in _request_wrapper
    response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(429,))
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_http.py", line 310, in http_backoff
    response = session.request(method=method, url=url, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\requests\adapters.py", line 688, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /deepseek-ai/deepseek-llm-7b-base/resolve/7683fea62db869066ddaff6a41d032262c490d4f/pytorch_model-00001-of-00002.bin (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000026C86D20CD0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: ec483f00-c906-4da7-b080-e26e1f95eaf7)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 439, in cached_files
    snapshot_download(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\_snapshot_download.py", line 297, in snapshot_download
    thread_map(
  File "D:\intelligent-ai\venv\lib\site-packages\tqdm\contrib\concurrent.py", line 69, in thread_map
    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\tqdm\contrib\concurrent.py", line 51, in _executor_map
    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))
  File "D:\intelligent-ai\venv\lib\site-packages\tqdm\std.py", line 1181, in __iter__
    for obj in iterable:
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\_base.py", line 458, in result
    return self.__get_result()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\_snapshot_download.py", line 271, in _inner_hf_hub_download
    return hf_hub_download(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1115, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1646, in _raise_on_head_call_error
    raise LocalEntryNotFoundError(
huggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\intelligent-ai\app\api\llm.py", line 28, in generate_pipeline
    result = await llm_service.generate(
  File "D:\intelligent-ai\app\services\llm_service.py", line 49, in generate
    runner = self.get_runner(model)
  File "D:\intelligent-ai\app\services\llm_service.py", line 40, in get_runner
    self.runners[model_name] = runner_cls(cfg)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 23, in __init__
    self._load_model_and_tokenizer()
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 95, in _load_model_and_tokenizer
    self.model = AutoModelForCausalLM.from_pretrained(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\modeling_utils.py", line 4260, in from_pretrained
    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\modeling_utils.py", line 1152, in _get_resolved_checkpoint_files
    checkpoint_files, sharded_metadata = get_checkpoint_shard_files(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 1115, in get_checkpoint_shard_files
    cached_filenames = cached_files(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 491, in cached_files
    raise OSError(
OSError: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.[0m
[92m[INFO] 2025-05-22 20:59:37 main:29 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-22 20:59:37 main:35 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-22 21:00:30 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:00:50 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:00:54 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-22 21:00:55 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-22 21:00:55 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-22 21:01:17 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:01:17 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:01:17 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-22 21:01:17 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-22 21:01:25 api.llm:25 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[91m[ERROR] 2025-05-22 21:08:25 api.llm:54 - Ошибка генерации
Traceback (most recent call last):
  File "D:\intelligent-ai\app\api\llm.py", line 28, in generate_pipeline
    result = await llm_service.generate(
  File "D:\intelligent-ai\app\services\llm_service.py", line 49, in generate
    runner = self.get_runner(model)
  File "D:\intelligent-ai\app\services\llm_service.py", line 40, in get_runner
    self.runners[model_name] = runner_cls(cfg)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 24, in __init__
    from app.core.config import config_store
ModuleNotFoundError: No module named 'app'[0m
[91m[ERROR] 2025-05-22 21:08:25 asyncio:1758 - Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)
handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>
Traceback (most recent call last):
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\asyncio\events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\asyncio\proactor_events.py", line 165, in _call_connection_lost
    self._sock.shutdown(socket.SHUT_RDWR)
ConnectionResetError: [WinError 10054] Удаленный хост принудительно разорвал существующее подключение[0m
[92m[INFO] 2025-05-22 21:08:58 api.llm:25 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[92m[INFO] 2025-05-22 21:09:28 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:09:52 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-22 21:09:54 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-22 21:09:54 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-22 21:09:57 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:09:57 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:09:57 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-22 21:09:57 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-22 21:10:12 api.llm:25 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[91m[ERROR] 2025-05-22 21:10:31 api.llm:54 - Ошибка генерации
Traceback (most recent call last):
  File "D:\intelligent-ai\app\api\llm.py", line 28, in generate_pipeline
    result = await llm_service.generate(
  File "D:\intelligent-ai\app\services\llm_service.py", line 60, in generate
    result = await runner.generate(prompt, **runtime_params)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 138, in generate
    res = await loop.run_in_executor(None, sync_gen)
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 126, in sync_gen
    result = self.text_generator(prompt, **final_gen_kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\text_generation.py", line 287, in __call__
    return super().__call__(text_inputs, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\base.py", line 1379, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\base.py", line 1386, in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\base.py", line 1286, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\text_generation.py", line 385, in _forward
    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\generation\utils.py", line 2225, in generate
    self._validate_model_kwargs(model_kwargs.copy())
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\generation\utils.py", line 1536, in _validate_model_kwargs
    raise ValueError(
ValueError: The following `model_kwargs` are not used by the model: ['device', 'torch_dtype', 'additionalProp1'] (note: typos in the generate arguments will also show up in this list)[0m
[92m[INFO] 2025-05-22 21:14:04 main:29 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-22 21:14:04 main:35 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-22 21:14:20 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:14:45 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-22 21:14:47 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-22 21:14:47 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-22 21:14:51 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:14:51 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:14:51 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-22 21:14:51 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-22 21:14:55 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[91m[ERROR] 2025-05-22 21:15:16 api.llm:57 - Ошибка генерации
Traceback (most recent call last):
  File "D:\intelligent-ai\app\api\llm.py", line 30, in generate_pipeline
    result = await llm_service.generate(
  File "D:\intelligent-ai\app\services\llm_service.py", line 60, in generate
    result = await runner.generate(prompt, **runtime_params)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 135, in generate
    final_gen_kwargs = self.filter_generate_kwargs(final_gen_kwargs)  # Фильтруем!
TypeError: TransformersRunner.filter_generate_kwargs() takes 1 positional argument but 2 were given[0m
[92m[INFO] 2025-05-22 21:15:48 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:16:58 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-22 21:17:09 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-22 21:17:09 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-22 21:17:17 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:17:18 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:17:18 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-22 21:17:18 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-22 21:17:22 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[91m[ERROR] 2025-05-22 21:18:14 api.llm:57 - Ошибка генерации
Traceback (most recent call last):
  File "D:\intelligent-ai\app\api\llm.py", line 30, in generate_pipeline
    result = await llm_service.generate(
  File "D:\intelligent-ai\app\services\llm_service.py", line 60, in generate
    result = await runner.generate(prompt, **runtime_params)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 152, in generate
    res = await loop.run_in_executor(None, sync_gen)
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 139, in sync_gen
    result = self.text_generator(prompt, **final_gen_kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\text_generation.py", line 287, in __call__
    return super().__call__(text_inputs, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\base.py", line 1379, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\base.py", line 1386, in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\base.py", line 1286, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\pipelines\text_generation.py", line 385, in _forward
    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\generation\utils.py", line 2358, in generate
    prepared_logits_processor = self._get_logits_processor(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\generation\utils.py", line 1194, in _get_logits_processor
    processors.append(TemperatureLogitsWarper(generation_config.temperature))
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\generation\logits_process.py", line 282, in __init__
    raise ValueError(except_msg)
ValueError: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.[0m
[92m[INFO] 2025-05-22 21:22:53 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:23:16 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-22 21:23:18 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-22 21:23:18 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-22 21:23:21 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:23:21 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-22 21:23:21 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-22 21:23:21 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-22 21:23:54 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[92m[INFO] 2025-05-22 21:28:28 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[92m[INFO] 2025-05-22 21:28:57 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[92m[INFO] 2025-05-22 21:29:20 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[92m[INFO] 2025-05-22 21:30:35 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[92m[INFO] 2025-05-22 21:31:31 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[92m[INFO] 2025-05-22 21:32:03 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[92m[INFO] 2025-05-22 21:32:39 api.llm:27 - Запрос генерации: model=deepseek-7b-base, user=string[0m
[92m[INFO] 2025-05-24 09:27:16 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:27:52 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:27:55 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:27:55 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:27:59 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:27:59 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:27:59 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 09:27:59 main:25 - Startup: Starting metrics persist background task.[0m
[91m[ERROR] 2025-05-24 09:29:26 main:55 - Unhandled exception: module 'services.retriever_service' has no attribute 'query'
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 177, in __call__
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 175, in __call__
    await self.app(scope, receive, send_wrapper)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 73, in app
    response = await f(request)
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "D:\intelligent-ai\app\api\rag.py", line 18, in rag_generate
    docs = retriever_service.query(req.question, top_k=req.top_k)
AttributeError: module 'services.retriever_service' has no attribute 'query'[0m
[92m[INFO] 2025-05-24 09:32:05 main:29 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 09:32:05 main:35 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 09:32:07 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:32:11 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:32:12 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:32:12 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:32:14 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:32:14 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:32:14 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 09:32:14 main:25 - Startup: Starting metrics persist background task.[0m
[91m[ERROR] 2025-05-24 09:33:02 main:55 - Unhandled exception: module 'services.rag_history_service' has no attribute 'log'
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 177, in __call__
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 175, in __call__
    await self.app(scope, receive, send_wrapper)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 73, in app
    response = await f(request)
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "D:\intelligent-ai\app\api\rag.py", line 37, in rag_generate
    await rag_history_service.log(
AttributeError: module 'services.rag_history_service' has no attribute 'log'[0m
[92m[INFO] 2025-05-24 09:33:22 main:29 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 09:33:22 main:35 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 09:33:30 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:33:52 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:33:53 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:33:53 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:33:56 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:33:56 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:33:56 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 09:33:56 main:25 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 09:38:58 main:29 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 09:38:58 main:35 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 09:39:03 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:39:21 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:39:22 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:39:22 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:39:24 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:39:25 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:39:25 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 09:39:25 main:25 - Startup: Starting metrics persist background task.[0m
[91m[ERROR] 2025-05-24 09:44:10 main:55 - Unhandled exception: 'deepseek-13b-base'
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 177, in __call__
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 175, in __call__
    await self.app(scope, receive, send_wrapper)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 73, in app
    response = await f(request)
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "D:\intelligent-ai\app\api\rag.py", line 27, in rag_generate
    runner = llm_service.get_runner(req.model)
  File "D:\intelligent-ai\app\services\llm_service.py", line 34, in get_runner
    cfg = config_store.get_model_config(model_name)
  File "D:\intelligent-ai\app\core\config.py", line 104, in get_model_config
    return self._models[name]
KeyError: 'deepseek-13b-base'[0m
[92m[INFO] 2025-05-24 09:45:10 main:29 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 09:45:10 main:35 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 09:45:13 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:45:20 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:45:43 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:48:20 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:48:47 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:48:49 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:48:49 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:48:52 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:48:52 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:48:52 main:23 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 09:48:52 main:25 - Startup: Starting metrics persist background task.[0m
[91m[ERROR] 2025-05-24 09:48:57 main:55 - Unhandled exception: deepseek-ai/deepseek-llm-13b-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "D:\intelligent-ai\venv\lib\site-packages\requests\models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/deepseek-ai/deepseek-llm-13b-base/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 424, in cached_files
    hf_hub_download(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1115, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1643, in _raise_on_head_call_error
    raise head_call_error
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1531, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1448, in get_hf_file_metadata
    r = _request_wrapper(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68316bd8-764a181a47541d232fa5b15d;83827964-53b6-490c-8d1f-8e6db6072618)

Repository Not Found for url: https://huggingface.co/deepseek-ai/deepseek-llm-13b-base/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 177, in __call__
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 175, in __call__
    await self.app(scope, receive, send_wrapper)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 73, in app
    response = await f(request)
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "D:\intelligent-ai\app\api\rag.py", line 27, in rag_generate
    runner = llm_service.get_runner(req.model)
  File "D:\intelligent-ai\app\services\llm_service.py", line 40, in get_runner
    self.runners[model_name] = runner_cls(cfg)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 24, in __init__
    self._load_model_and_tokenizer()
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 127, in _load_model_and_tokenizer
    self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.model_path)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\models\auto\tokenization_auto.py", line 946, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\models\auto\tokenization_auto.py", line 778, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 456, in cached_files
    raise OSError(
OSError: deepseek-ai/deepseek-llm-13b-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`[0m
[92m[INFO] 2025-05-24 09:51:08 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:51:12 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:51:12 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:51:12 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:51:15 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:51:15 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:51:15 main:24 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 09:51:15 main:26 - Startup: Starting metrics persist background task.[0m
[91m[ERROR] 2025-05-24 09:51:21 main:56 - Unhandled exception: deepseek-ai/deepseek-llm-13b-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "D:\intelligent-ai\venv\lib\site-packages\requests\models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/deepseek-ai/deepseek-llm-13b-base/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 424, in cached_files
    hf_hub_download(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1115, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1643, in _raise_on_head_call_error
    raise head_call_error
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1531, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 1448, in get_hf_file_metadata
    r = _request_wrapper(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "D:\intelligent-ai\venv\lib\site-packages\huggingface_hub\utils\_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68316c68-512af42d19d21bd15000a2e4;30a47c32-28d4-41ba-a3b7-8948c61f7706)

Repository Not Found for url: https://huggingface.co/deepseek-ai/deepseek-llm-13b-base/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 177, in __call__
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 175, in __call__
    await self.app(scope, receive, send_wrapper)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 73, in app
    response = await f(request)
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "D:\intelligent-ai\app\api\rag.py", line 27, in rag_generate
    runner = llm_service.get_runner(req.model)
  File "D:\intelligent-ai\app\services\llm_service.py", line 40, in get_runner
    self.runners[model_name] = runner_cls(cfg)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 24, in __init__
    self._load_model_and_tokenizer()
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 127, in _load_model_and_tokenizer
    self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.model_path)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\models\auto\tokenization_auto.py", line 946, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\models\auto\tokenization_auto.py", line 778, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 456, in cached_files
    raise OSError(
OSError: deepseek-ai/deepseek-llm-13b-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`[0m
[92m[INFO] 2025-05-24 09:56:19 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:56:23 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:56:24 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:56:24 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:56:26 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:56:48 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:56:52 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:56:52 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:56:52 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:56:54 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:57:17 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:57:21 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:57:21 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:57:21 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:57:23 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:57:23 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:57:23 main:24 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 09:57:23 main:26 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 09:58:07 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:58:11 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:58:11 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:58:11 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:58:13 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:58:13 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:58:13 main:28 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 09:58:13 main:30 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 09:58:20 main:34 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 09:58:20 main:40 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 09:58:27 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:58:31 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 09:58:32 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 09:58:32 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 09:58:34 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:58:34 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 09:58:34 main:30 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 09:58:34 main:32 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 09:59:56 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:00:00 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 10:00:00 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 10:00:00 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 10:00:02 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:00:02 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:00:02 main:32 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 10:00:02 main:34 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 10:00:28 main:38 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 10:00:28 main:44 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 10:00:30 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:00:34 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 10:00:34 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 10:00:34 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 10:00:36 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:00:36 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:00:36 main:34 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 10:00:36 main:36 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 10:01:25 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:01:29 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 10:01:30 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 10:01:30 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 10:01:32 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:01:32 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:01:32 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 10:01:32 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 10:02:01 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:02:05 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 10:02:05 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 10:02:05 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 10:02:07 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:02:07 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:02:07 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 10:02:07 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 10:02:36 main:44 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 10:02:36 main:50 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 10:02:38 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:02:42 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 10:02:42 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 10:02:42 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 10:02:44 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:02:44 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:02:44 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 10:02:44 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 10:03:52 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:03:56 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 10:03:56 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 10:03:56 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 10:03:59 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:03:59 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:03:59 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 10:03:59 main:40 - Startup: Starting metrics persist background task.[0m
[91m[ERROR] 2025-05-24 10:06:34 main:70 - Unhandled exception: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B does not appear to have files named ('model-00001-of-000004.safetensors', 'model-00002-of-000004.safetensors', 'model-00003-of-000004.safetensors', 'model-00004-of-000004.safetensors'). Checkout 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B/tree/main'for available files.
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 177, in __call__
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 175, in __call__
    await self.app(scope, receive, send_wrapper)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 73, in app
    response = await f(request)
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "D:\intelligent-ai\app\api\rag.py", line 27, in rag_generate
    runner = llm_service.get_runner(req.model)
  File "D:\intelligent-ai\app\services\llm_service.py", line 40, in get_runner
    self.runners[model_name] = runner_cls(cfg)
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 24, in __init__
    self._load_model_and_tokenizer()
  File "D:\intelligent-ai\app\llm_runners\transformers.py", line 128, in _load_model_and_tokenizer
    self.model = AutoModelForCausalLM.from_pretrained(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\modeling_utils.py", line 4260, in from_pretrained
    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\modeling_utils.py", line 1152, in _get_resolved_checkpoint_files
    checkpoint_files, sharded_metadata = get_checkpoint_shard_files(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 1115, in get_checkpoint_shard_files
    cached_filenames = cached_files(
  File "D:\intelligent-ai\venv\lib\site-packages\transformers\utils\hub.py", line 517, in cached_files
    raise EnvironmentError(
OSError: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B does not appear to have files named ('model-00001-of-000004.safetensors', 'model-00002-of-000004.safetensors', 'model-00003-of-000004.safetensors', 'model-00004-of-000004.safetensors'). Checkout 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B/tree/main'for available files.[0m
[92m[INFO] 2025-05-24 10:06:37 main:44 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 10:06:37 main:50 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 10:06:41 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:06:45 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 10:06:45 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 10:06:45 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[93m[WARNING] 2025-05-24 10:06:47 huggingface_hub.file_download:1717 - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`[0m
[92m[INFO] 2025-05-24 10:06:53 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:06:53 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:06:53 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 10:06:53 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 10:23:31 main:44 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 10:23:31 main:50 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 10:25:23 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:25:30 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:25:59 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 10:26:02 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 10:26:02 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 10:26:06 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:26:06 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 10:26:06 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 10:26:06 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 10:31:39 main:44 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 10:31:39 main:50 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 11:26:08 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:26:37 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 11:26:39 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 11:26:39 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 11:26:42 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:26:43 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:26:43 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 11:26:43 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 11:46:42 main:44 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 11:46:42 main:50 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 11:46:44 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:46:48 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 11:46:49 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 11:46:49 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 11:46:51 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:46:51 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:46:51 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 11:46:51 main:40 - Startup: Starting metrics persist background task.[0m
[91m[ERROR] 2025-05-24 11:47:14 main:70 - Unhandled exception: "AppConfig" object has no field "additionalProp1"
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 177, in __call__
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 175, in __call__
    await self.app(scope, receive, send_wrapper)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 73, in app
    response = await f(request)
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "D:\intelligent-ai\app\api\admin.py", line 54, in set_app_config
    setattr(config_store.app_config, k, v)
  File "D:\intelligent-ai\venv\lib\site-packages\pydantic\main.py", line 995, in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
  File "D:\intelligent-ai\venv\lib\site-packages\pydantic\main.py", line 1042, in _setattr_handler
    raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
ValueError: "AppConfig" object has no field "additionalProp1"[0m
[92m[INFO] 2025-05-24 11:47:36 main:44 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 11:47:36 main:50 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 11:49:22 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:49:27 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 11:49:27 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 11:49:27 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 11:49:29 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:49:29 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:49:29 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 11:49:29 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 11:55:38 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:55:43 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 11:55:43 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 11:55:43 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 11:55:46 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:55:46 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 11:55:46 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 11:55:46 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 11:57:20 api.llm:29 - Запрос генерации: model=, user=None[0m
[91m[ERROR] 2025-05-24 11:57:20 api.llm:60 - Ошибка генерации
Traceback (most recent call last):
  File "D:\intelligent-ai\app\api\llm.py", line 32, in generate_pipeline
    result = await llm_service.generate(
  File "D:\intelligent-ai\app\services\llm_service.py", line 48, in generate
    cfg = config_store.get_model_config(model)
  File "D:\intelligent-ai\app\core\config.py", line 104, in get_model_config
    return self._models[name]
KeyError: ''[0m
[92m[INFO] 2025-05-24 11:57:57 api.llm:29 - Запрос генерации: model=, user=None[0m
[91m[ERROR] 2025-05-24 11:57:57 api.llm:60 - Ошибка генерации
Traceback (most recent call last):
  File "D:\intelligent-ai\app\api\llm.py", line 32, in generate_pipeline
    result = await llm_service.generate(
  File "D:\intelligent-ai\app\services\llm_service.py", line 48, in generate
    cfg = config_store.get_model_config(model)
  File "D:\intelligent-ai\app\core\config.py", line 104, in get_model_config
    return self._models[name]
KeyError: ''[0m
[92m[INFO] 2025-05-24 11:59:01 main:44 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 11:59:01 main:50 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 12:01:27 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:01:31 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 12:01:32 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 12:01:32 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 12:01:34 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:01:34 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:01:34 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 12:01:34 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 12:04:30 api.llm:29 - Запрос генерации: model=, user=None[0m
[91m[ERROR] 2025-05-24 12:04:30 api.llm:60 - Ошибка генерации
Traceback (most recent call last):
  File "D:\intelligent-ai\app\api\llm.py", line 32, in generate_pipeline
    result = await llm_service.generate(
  File "D:\intelligent-ai\app\services\llm_service.py", line 48, in generate
    cfg = config_store.get_model_config(model)
  File "D:\intelligent-ai\app\core\config.py", line 104, in get_model_config
    return self._models[name]
KeyError: ''[0m
[92m[INFO] 2025-05-24 12:05:35 main:44 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 12:05:35 main:50 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 12:05:37 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:05:41 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 12:05:42 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 12:05:42 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 12:05:44 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:05:44 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:05:44 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 12:05:44 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 12:05:53 api.llm:29 - Запрос генерации: model=deepseek-r1-qwen-14b, user=None[0m
[92m[INFO] 2025-05-24 12:10:04 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:10:29 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 12:10:33 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 12:10:33 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 12:10:37 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:10:37 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:10:37 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 12:10:37 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 12:10:46 api.llm:29 - Запрос генерации: model=deepseek-r1-qwen-14b, user=None[0m
[92m[INFO] 2025-05-24 12:12:58 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:13:29 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 12:13:31 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 12:13:31 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 12:13:34 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:13:35 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:13:35 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 12:13:35 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 12:15:58 api.llm:29 - Запрос генерации: model=deepseek-r1-qwen-14b, user=None[0m
[92m[INFO] 2025-05-24 12:17:53 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:19:18 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 12:19:28 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 12:19:28 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 12:19:34 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:19:35 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:19:35 main:38 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 12:19:35 main:40 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 12:21:27 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:21:32 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 12:21:32 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 12:21:32 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 12:21:34 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:21:34 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:21:34 main:39 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 12:21:34 main:41 - Startup: Starting metrics persist background task.[0m
[91m[ERROR] 2025-05-24 12:21:37 main:71 - Unhandled exception: name 'FileResponse' is not defined
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 177, in __call__
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 175, in __call__
    await self.app(scope, receive, send_wrapper)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 73, in app
    response = await f(request)
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "D:\intelligent-ai\app\main.py", line 89, in serve_spa
    return FileResponse(index_path)
NameError: name 'FileResponse' is not defined. Did you mean: 'JSONResponse'?[0m
[91m[ERROR] 2025-05-24 12:21:37 main:71 - Unhandled exception: name 'FileResponse' is not defined
Traceback (most recent call last):
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 177, in __call__
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\prometheus_fastapi_instrumentator\middleware.py", line 175, in __call__
    await self.app(scope, receive, send_wrapper)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "D:\intelligent-ai\venv\lib\site-packages\starlette\routing.py", line 73, in app
    response = await f(request)
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "D:\intelligent-ai\venv\lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "D:\intelligent-ai\app\main.py", line 89, in serve_spa
    return FileResponse(index_path)
NameError: name 'FileResponse' is not defined. Did you mean: 'JSONResponse'?[0m
[92m[INFO] 2025-05-24 12:21:48 main:45 - Shutdown: Cancelling metrics persist task.[0m
[92m[INFO] 2025-05-24 12:21:48 main:51 - Shutdown: Persist task stopped.[0m
[92m[INFO] 2025-05-24 12:21:50 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:21:54 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 12:21:54 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 12:21:54 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 12:21:56 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:21:56 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:21:56 main:39 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 12:21:56 main:41 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 12:22:07 api.llm:29 - Запрос генерации: model=deepseek-r1-qwen-14b, user=None[0m
[92m[INFO] 2025-05-24 12:23:25 api.llm:29 - Запрос генерации: model=deepseek-r1-qwen-14b, user=None[0m
[92m[INFO] 2025-05-24 12:23:56 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:24:19 datasets:54 - PyTorch version 2.5.1+cu121 available.[0m
[92m[INFO] 2025-05-24 12:24:21 sentence_transformers.SentenceTransformer:211 - Use pytorch device_name: cuda:0[0m
[92m[INFO] 2025-05-24 12:24:21 sentence_transformers.SentenceTransformer:219 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2[0m
[92m[INFO] 2025-05-24 12:24:24 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:24:24 root:77 - Logging setup: level=INFO, format=color, file=logs/app.log[0m
[92m[INFO] 2025-05-24 12:24:24 main:39 - Startup: Restoring metrics from last snapshot...[0m
[92m[INFO] 2025-05-24 12:24:24 main:41 - Startup: Starting metrics persist background task.[0m
[92m[INFO] 2025-05-24 12:24:27 api.llm:29 - Запрос генерации: model=deepseek-r1-qwen-14b, user=string[0m
[92m[INFO] 2025-05-24 12:26:21 api.llm:29 - Запрос генерации: model=deepseek-r1-qwen-14b, user=None[0m
