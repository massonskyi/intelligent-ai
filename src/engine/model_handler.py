import asyncio
import functools
import json
import logging
import os
import time
from typing import Callable, Final, Optional
from concurrent.futures import ThreadPoolExecutor
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
from pygments import highlight
from pygments.lexers import GroovyLexer
from pygments.formatters import TerminalFormatter
from src.settings import get_settings
from typing import final
import torch

def time_it(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"Time taken for {func.__name__}: {end_time - start_time:.2f} seconds")
        return result
    return wrapper



@final
class JenkinsPipelineGenerator:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ Jenkins pipeline —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–∫."""
        self.settings = get_settings()
        self.model = None
        self.tokenizer = None
        self.executor = ThreadPoolExecutor(max_workers=2)
        self.log = logging.getLogger(__name__)
        # –ó–∞–¥–∞—ë–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ, –∏–Ω–∞—á–µ CPU)
        self.device: Final = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.log.info(f"Device: {self.device}")
    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –º–æ–¥–µ–ª—å –∏ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –µ—ë –≤ LoRA."""
        @time_it
        def load_and_wrap():
            self.log.info("Loading tokenizer and model...")
            # 1) –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            tok = AutoTokenizer.from_pretrained(f"{os.getcwd()}/codet5p_finetuned")
            self.log.info("Loading base_model...")
            # 2) –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
            base_model = AutoModelForSeq2SeqLM.from_pretrained(f"{os.getcwd()}/codet5p_finetuned")
            self.log.info("Loading LoRA config...")
            # 3) –ö–æ–Ω—Ñ–∏–≥ LoRA
            lora_cfg = LoraConfig(
                r=8,
                lora_alpha=32,
                target_modules=["q", "v"],   # –∏–ª–∏ —Ç–æ—á–Ω—ã–µ –∏–º–µ–Ω–∞ —Å–ª–æ—ë–≤: ["q_proj","v_proj"]
                lora_dropout=0.1,
                bias="none",
                task_type="SEQ_2_SEQ_LM",
            )

            # 4) –û–±–æ—Ä–∞—á–∏–≤–∞–Ω–∏–µ
            lora_model = get_peft_model(base_model, lora_cfg)

            # 5) –ü–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            lora_model.to(self.device)

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ø–æ—Å—á–∏—Ç–∞–µ–º –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            total = sum(p.numel() for p in lora_model.parameters())
            trainable = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)
            print(f"Total params: {total:,}, trainable: {trainable:,}")

            return tok, lora_model

        loop = asyncio.get_event_loop()
        self.tokenizer, self.model = await loop.run_in_executor(self.executor, load_and_wrap)

   # ------------------------- generate_pipeline -------------------------
    @time_it
    async def generate_pipeline(
        self,
        input_json: dict,
        callback: Optional[Callable[[str], None]] = None,
    ) -> str:

        input_text = f"{json.dumps(input_json)}"

        # -- 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ü–ï–†–ï–ù–û–° –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üîë
        def _tokenize_to_device(txt: str):
            enc = self.tokenizer(
                txt,
                return_tensors="pt",
                max_length=self.settings.MAX_LENGTH,
                truncation=True,
            )
            # enc ‚Äî —ç—Ç–æ dict; –ø–µ—Ä–µ–Ω–æ—Å–∏–º –∫–∞–∂–¥—ã–π —Ç–µ–Ω–∑–æ—Ä
            return {k: v.to(self.device) for k, v in enc.items()}

        loop = asyncio.get_event_loop()
        inputs = await loop.run_in_executor(self.executor,
                                            lambda: _tokenize_to_device(input_text))

        # -- 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (—Ç–µ–ø–µ—Ä—å –∏ –º–æ–¥–µ–ª—å, –∏ inputs ‚Äî –Ω–∞ –æ–¥–Ω–æ–º –¥–µ–≤–∞–π—Å–µ)
        start = time.time()
        outputs = await loop.run_in_executor(
            self.executor,
            lambda: self.model.generate(
                **inputs,
                max_length=self.settings.MAX_LENGTH,
                num_beams=5,
            ),
        )
        if self.settings.DEBUG:
            print(f"Generation time: {time.time() - start:.2f}s")

        # -- 3. –î–µ–∫–æ–¥–∏—Ä—É–µ–º –Ω–∞ CPU (–∏–Ω–∞—á–µ tokenizer.decode —Ä—É–≥–∞–µ—Ç—Å—è)
        generated_ids = outputs[0].detach().cpu()           # üîë
        pipeline = await loop.run_in_executor(
            self.executor,
            lambda: self.tokenizer.decode(generated_ids, skip_special_tokens=True),
        )

        if callback:
            callback(pipeline)
        return pipeline

    async def demo_formatter(self, pipeline_text: str, callback: Optional[Callable[[str], None]] = None) -> str:
        """
        –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏).

           Args:
            pipeline_text: –¢–µ–∫—Å—Ç pipeline –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
            callback: –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞.

        Returns:
            str: –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π pipeline.
        """
        if self.settings.DEBUG:
            print("=== –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç ===")
            print(pipeline_text)
            print("\n=== –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π pipeline ===")

        highlighted = highlight(pipeline_text, GroovyLexer(), TerminalFormatter())
        print(highlighted)

        if callback:
            callback(pipeline_text)

        return pipeline_text

    def __del__(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        self.executor.shutdown(wait=True)